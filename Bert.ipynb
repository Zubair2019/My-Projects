{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGtCbP4XJ84L"
      },
      "source": [
        "## Fine-tuning a pretrained Bert Model fro Classification using Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL1nkJfqjuDZ"
      },
      "source": [
        "Install Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmLwXqMEKyvE"
      },
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lLuDf8-jzlf"
      },
      "source": [
        "Before finetuning a model we need the dataset with which to finetune the model. For this tutorial we will use the imdb dataset to classify a movie reviews as positive and negative. The raw_datasets object is a dictionary with three keys: \"train\", \"test\" and \"unsupervised\" (which correspond to the three splits of that dataset). We will use the \"train\" split for training and the \"test\" split for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYffPLZpKHZt"
      },
      "source": [
        "from datasets import load_dataset\n",
        "raw_dataset = load_dataset('imdb')\n",
        "print(raw_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAzOqqofnVxo"
      },
      "source": [
        "Now we will use the AutoTokenizer from the transformers library to preprocess our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZeOBZyhKqxp"
      },
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer  = AutoTokenizer.from_pretrained('bert-base-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kddGA7esoIN0"
      },
      "source": [
        "We will use the map method for pre-processing all the splits of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U-9HILnLRZ-"
      },
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = raw_dataset.map(tokenize_function, batched=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBPjBANModvd"
      },
      "source": [
        "Lets keep a small dataset for finetuning. We will use this dataset for finetuning in this tutorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwQ2GwRiOWhE"
      },
      "source": [
        "small_train_dataset = tokenized_datasets['train'].shuffle(seed = 42).select(range(1000))\n",
        "small_test_dataset = tokenized_datasets['test'].shuffle(seed = 42).select(range(1000))\n",
        "full_train_dataset = tokenized_datasets['train']\n",
        "full_test_datasets = tokenized_datasets['test']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6wfbh1Xd-4y"
      },
      "source": [
        "import pandas as pd\n",
        "print(small_train_dataset['label'][0:5],'\\n')\n",
        "df = pd.DataFrame(small_train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns58MKdNfrRZ"
      },
      "source": [
        "df.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dtayt6AIpNva"
      },
      "source": [
        "Since our downstream task is a cassification task while the Bert has been pre-trained for text Generation task, we will use the \"BertForClassification\" model for our down stream task. This means we are removing the Pre-trained Head of Bert and replacing it with the Classification Head. This apparently means some of the weights will be randomly initialized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JbHrhrgPKUB"
      },
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased',num_labels = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8FZw1c9PKrC"
      },
      "source": [
        "from transformers import TrainingArguments\n",
        "training_args = TrainingArguments('test_trainer', evaluation_strategy='epoch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ObcxaPBRXkb"
      },
      "source": [
        "from transformers import Trainer\n",
        "trainer = Trainer(model = model, args = training_args, train_dataset = small_train_dataset, eval_dataset = small_test_dataset, compute_metrics = compute_metrics)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iwUI13yRXnv"
      },
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4nX-CD3Uktw"
      },
      "source": [
        "trainer.train()\n",
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1FET25NUotg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}